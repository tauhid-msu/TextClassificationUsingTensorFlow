{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce7f3d12",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "ce7f3d12",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import csv\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51d18e5b",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [
          "graded"
        ],
        "id": "51d18e5b"
      },
      "outputs": [],
      "source": [
        "with open(\"./bbc-text.csv\", 'r') as csvfile:\n",
        "    print(f\"First line (header) looks like this:\\n\\n{csvfile.readline()}\")\n",
        "    print(f\"Each data point looks like this:\\n\\n{csvfile.readline()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87fb4a2a",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [
          "graded"
        ],
        "id": "87fb4a2a"
      },
      "outputs": [],
      "source": [
        "NUM_WORDS = 1000\n",
        "EMBEDDING_DIM = 16\n",
        "MAXLEN = 120\n",
        "PADDING = 'post'\n",
        "OOV_TOKEN = \"<OOV>\"\n",
        "TRAINING_SPLIT = .8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b10206ea",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [
          "graded"
        ],
        "id": "b10206ea"
      },
      "outputs": [],
      "source": [
        "def remove_stopwords(sentence):\n",
        "    # List of stopwords\n",
        "    stopwords = [\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]\n",
        "    sentence = sentence.lower()\n",
        "    words = sentence.split()\n",
        "    no_words = [w for w in words if w not in stopwords]\n",
        "    sentence = \" \".join(no_words)\n",
        "    return sentence\n",
        "\n",
        "def parse_data_from_file(filename):\n",
        "    sentences = []\n",
        "    labels = []\n",
        "    with open(filename, 'r') as csvfile:\n",
        "        reader = csv.reader(csvfile, delimiter=',')\n",
        "        next(reader)\n",
        "        for row in reader:\n",
        "            labels.append(row[0])\n",
        "            sentence = row[1]\n",
        "            sentence = remove_stopwords(sentence)\n",
        "            sentences.append(sentence)\n",
        "\n",
        "    return sentences, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f92f7fc",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [
          "graded"
        ],
        "id": "7f92f7fc"
      },
      "outputs": [],
      "source": [
        "sentences, labels = parse_data_from_file(\"./bbc-text.csv\")\n",
        "print(f\"There are {len(sentences)} sentences in the dataset.\\n\")\n",
        "print(f\"First sentence has {len(sentences[0].split())} words (after removing stopwords).\\n\")\n",
        "print(f\"There are {len(labels)} labels in the dataset.\\n\")\n",
        "print(f\"The first 5 labels are {labels[:5]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e0158cb",
      "metadata": {
        "deletable": false,
        "tags": [
          "graded"
        ],
        "id": "1e0158cb"
      },
      "outputs": [],
      "source": [
        "def train_val_split(sentences, labels, training_split):\n",
        "    train_size = int(len(sentences)*training_split)\n",
        "\n",
        "    train_sentences = sentences[0:train_size]\n",
        "    train_labels = labels[0:train_size]\n",
        "\n",
        "    validation_sentences = sentences[train_size:]\n",
        "    validation_labels = labels[train_size:]\n",
        "\n",
        "    return train_sentences, validation_sentences, train_labels, validation_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc283035",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [
          "graded"
        ],
        "id": "cc283035"
      },
      "outputs": [],
      "source": [
        "train_sentences, val_sentences, train_labels, val_labels = train_val_split(sentences, labels, TRAINING_SPLIT)\n",
        "\n",
        "print(f\"There are {len(train_sentences)} sentences for training.\\n\")\n",
        "print(f\"There are {len(train_labels)} labels for training.\\n\")\n",
        "print(f\"There are {len(val_sentences)} sentences for validation.\\n\")\n",
        "print(f\"There are {len(val_labels)} labels for validation.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af18d135",
      "metadata": {
        "deletable": false,
        "lines_to_next_cell": 2,
        "tags": [
          "graded"
        ],
        "id": "af18d135"
      },
      "outputs": [],
      "source": [
        "def fit_tokenizer(train_sentences, numWords, oovToken):\n",
        "    tokenizer = Tokenizer(num_words = numWords, oov_token=oovToken)\n",
        "    tokenizer.fit_on_texts(train_sentences)\n",
        "    return tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07772b6f",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [
          "graded"
        ],
        "id": "07772b6f"
      },
      "outputs": [],
      "source": [
        "tokenizer = fit_tokenizer(train_sentences, NUM_WORDS, OOV_TOKEN)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "print(f\"Vocabulary contains {len(word_index)} words\\n\")\n",
        "print(\"<OOV> token included in vocabulary\" if \"<OOV>\" in word_index else \"<OOV> token NOT included in vocabulary\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8eec1dd6",
      "metadata": {
        "deletable": false,
        "tags": [
          "graded"
        ],
        "id": "8eec1dd6"
      },
      "outputs": [],
      "source": [
        "def seq_and_pad(sentences, tokenizer, paddingType, maxLen):\n",
        "    # Convert sentences to sequences\n",
        "    sequences = tokenizer.texts_to_sequences(sentences)\n",
        "    # Pad the sequences using the correct padding and maxlen\n",
        "    padded_sequences = pad_sequences(sequences, maxlen=maxLen, padding=paddingType)\n",
        "    return padded_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b0ff145",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [
          "graded"
        ],
        "id": "2b0ff145"
      },
      "outputs": [],
      "source": [
        "train_padded_seq = seq_and_pad(train_sentences, tokenizer, PADDING, MAXLEN)\n",
        "val_padded_seq = seq_and_pad(val_sentences, tokenizer, PADDING, MAXLEN)\n",
        "\n",
        "print(f\"Padded training sequences have shape: {train_padded_seq.shape}\\n\")\n",
        "print(f\"Padded validation sequences have shape: {val_padded_seq.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "217c08ad",
      "metadata": {
        "deletable": false,
        "id": "217c08ad",
        "lines_to_next_cell": 2,
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "def tokenize_labels(all_labels, split_labels):\n",
        "    label_tokenizer = Tokenizer(num_words = NUM_WORDS)\n",
        "    label_tokenizer.fit_on_texts(all_labels)\n",
        "    label_seq = label_tokenizer.texts_to_sequences(split_labels)\n",
        "    label_seq_np = np.array(label_seq)-1\n",
        "    return label_seq_np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4c970e3",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [
          "graded"
        ],
        "id": "a4c970e3"
      },
      "outputs": [],
      "source": [
        "train_label_seq = tokenize_labels(labels, train_labels)\n",
        "val_label_seq = tokenize_labels(labels, val_labels)\n",
        "\n",
        "print(f\"First 5 labels of the training set should look like this:\\n{train_label_seq[:5]}\\n\")\n",
        "print(f\"First 5 labels of the validation set should look like this:\\n{val_label_seq[:5]}\\n\")\n",
        "print(f\"Tokenized labels of the training set have shape: {train_label_seq.shape}\\n\")\n",
        "print(f\"Tokenized labels of the validation set have shape: {val_label_seq.shape}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03a7fbc9",
      "metadata": {
        "deletable": false,
        "id": "03a7fbc9",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "def create_model(num_words, embedding_dim, maxlen):\n",
        "    tf.random.set_seed(123)\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(num_words, embedding_dim, input_length=maxlen),\n",
        "        tf.keras.layers.GlobalAveragePooling1D(),\n",
        "        tf.keras.layers.Dense(6, activation='relu'),\n",
        "        tf.keras.layers.Dense(5, activation='softmax')\n",
        "    ])\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa115b83",
      "metadata": {
        "tags": [],
        "id": "aa115b83"
      },
      "outputs": [],
      "source": [
        "model = create_model(NUM_WORDS, EMBEDDING_DIM, MAXLEN)\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(train_padded_seq, train_label_seq, epochs=30, validation_data=(val_padded_seq, val_label_seq))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "357f1ba4",
      "metadata": {
        "tags": [],
        "id": "357f1ba4"
      },
      "outputs": [],
      "source": [
        "def plot_graphs(history, metric):\n",
        "    plt.plot(history.history[metric])\n",
        "    plt.plot(history.history[f'val_{metric}'])\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(metric)\n",
        "    plt.legend([metric, f'val_{metric}'])\n",
        "    plt.show()\n",
        "\n",
        "plot_graphs(history, \"accuracy\")\n",
        "plot_graphs(history, \"loss\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ad672c8",
      "metadata": {
        "id": "0ad672c8",
        "tags": [],
        "outputId": "6dd70307-2272-40e1-e479-9ebd0bd8429f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Weights of embedding layer have shape: (1000, 16)\n"
          ]
        }
      ],
      "source": [
        "# Reverse word index\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "e = model.layers[0]\n",
        "weights = e.get_weights()[0]\n",
        "print(f\"Weights of embedding layer have shape: {weights.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77e96202",
      "metadata": {
        "id": "77e96202",
        "tags": []
      },
      "outputs": [],
      "source": [
        "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
        "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
        "for word_num in range(1, NUM_WORDS):\n",
        "    word = reverse_word_index[word_num]\n",
        "    embeddings = weights[word_num]\n",
        "    out_m.write(word + \"\\n\")\n",
        "    out_v.write('\\t'.join([str(x) for x in embeddings]) + \"\\n\")\n",
        "out_v.close()\n",
        "out_m.close()"
      ]
    }
  ],
  "metadata": {
    "dlai_version": "1.2.0",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}